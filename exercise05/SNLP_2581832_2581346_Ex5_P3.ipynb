{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNLP_2581832_2581346 Ex5 P3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5FUQVpIX0xH",
        "colab_type": "code",
        "outputId": "b180c7bf-7971-47d5-e1de-2f3eab7fcdad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "FILES = [\"corpus.bg\", \"corpus.de\", \"corpus.fi\", \"corpus.fr\"]\n",
        "\n",
        "def calculate_oov(vocabulary, length, testdata):\n",
        "\n",
        "\ttest_words = vocabulary\n",
        "\t\n",
        "\t#Count for number of unseen words in test corpus\n",
        "\tcount = 0\n",
        "\tflag = 0\n",
        "\tfor i in range(0,len(test_words)-1):\n",
        "\t\tfor j in range(0, len(vocabulary)-1):\n",
        "\t\t\tif(test_words[i]== vocabulary[j]):\n",
        "\t\t\t\tflag = 1\n",
        "\t\t\t\tbreak\n",
        "\t\tif(flag == 0):\n",
        "      #Calculating the number of unseen words\n",
        "\t\t\tcount = count + 1 \n",
        "\t\tflag = 0\n",
        "\treturn(count / length)\n",
        "\n",
        "\n",
        "for file, index in enumerate(FILES):\n",
        "    fileData = open(file, 'r').read()\n",
        "    # to lower, and tokenizing\n",
        "    tokens = re.findall(r'(\\b[A-Za-z]+[a-z]?\\b)', text_file.read())\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    \n",
        "\n",
        "    # Part 3a. splitting database\n",
        "    trainingWordTokens = list(tokens[index] for index in range(0, int(len(tokens) * 0.8)))\n",
        "    testWordTokens = list(tokens[index] for index in range(int(len(tokens) * 0.8), len(tokens)))\n",
        "\n",
        "    # Part 3b. \n",
        "    length = len(trainingWordTokens)\n",
        "    tokens_frequency = {}\n",
        "    #Calculating the frequency for each word in text\n",
        "    for word in tokens:\n",
        "      count = tokens_frequency.get(word,0)\n",
        "      tokens_frequency[word] = count + 1\n",
        "      #Sorting the frequencies\n",
        "    vocabulary = []\n",
        "    for key, value in reversed(sorted(tokens_frequency.items(), key = itemgetter(1))):\n",
        "      vocabulary.append(key)\t\n",
        "      #Remove duplicate\n",
        "    vocabulary = list(set(vocabulary))\n",
        "    print (vocabulary, length)\n",
        "\n",
        "    #Part 3c.\n",
        "    countlentgh = []\n",
        "    countlentgh[index] = calculate_oov(vocabulary, length, testWordTokens)\n",
        "    print (\"3c@: \", countlentgh[index])\n",
        "\n",
        "\n",
        "    plt.loglog(lentgh, countlentgh, label=FILE)\n",
        "\n",
        "\n",
        "rateOOV = plt.figure(1)\n",
        "plt.xlabel(\"Size of Vocabulary\")\n",
        "plt.ylabel(\"Rate of OOV\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-771160282373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfileData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# to lower, and tokenizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(\\b[A-Za-z]+[a-z]?\\b)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H191NgsKhj8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_OTzP3lfydy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}