{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNLP Assn 7",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtgXX4BVO8NS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5bd5aff3-9df8-41e4-8f15-cf0077de7394"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank\n",
        "import string\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import math\n",
        "import random"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJTN_xPJbWQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocess the text and eliminate words belonging to the corresponding tags mentioned (Task a)\n",
        "def pre_process(text):\n",
        "  pro_words = [(x.lower(), y) for (x,y) in corp if y not in ('NN', ',', '.', 'CD', 'LS', 'SENT', 'SYM', '#', '$', '-LRB-', '-RRB-', ':', '-NONE-', '\"', '“', '”')]\n",
        "  print('Pre-processed Corpus: {}\\n'.format(pro_words[:250]))\n",
        "  print('Length of the corpus: {}'.format(len(pro_words)))\n",
        "  #Split the corpus into trainset (80%) and testset (20%)\n",
        "  trainset = pro_words[:(len(pro_words) // 10) * 8]\n",
        "  testset = pro_words[(len(pro_words) // 10) * 8:]\n",
        "  print('Length of Trainset: {}'.format(len(trainset)))\n",
        "  print('Length of Testset: {}\\n'.format(len(testset)))\n",
        "  return trainset, testset\n",
        "\n",
        "#Functions to generate Unigrams, Bigrams and corresponding probabilities\n",
        "def total_count(ngramcount):\n",
        "    N = 0\n",
        "    for nGram in ngramcount:\n",
        "        N = N + ngramcount[nGram]\n",
        "    return N\n",
        "\n",
        "def get_word_count(words):\n",
        "    WordCount = {}\n",
        "    for i in range(len(words) - 1):\n",
        "        x = (words[i])\n",
        "        if not x in WordCount:\n",
        "            WordCount[x] = 1\n",
        "        else:\n",
        "            WordCount[x] += 1\n",
        "    return WordCount\n",
        "    \n",
        "def get_word_unigram_count(words):\n",
        "    unigramCount = {}\n",
        "    for i in range(len(words) - 1):\n",
        "        x = (words[i][0])\n",
        "        if not x in unigramCount:\n",
        "            unigramCount[x] = 1\n",
        "        else:\n",
        "            unigramCount[x] += 1\n",
        "    return unigramCount\n",
        "\n",
        "def get_word_bigram_count(words):\n",
        "    bigramCount = {}\n",
        "    for i in range(len(words) - 1):\n",
        "        x = (words[i][0], words[i+1][0])\n",
        "        if not x in bigramCount:\n",
        "            bigramCount[x] = 1\n",
        "        else:\n",
        "            bigramCount[x] += 1\n",
        "    return bigramCount\n",
        "\n",
        "def get_pos_unigram_count(words):\n",
        "    Counts = {}\n",
        "    for i in range(len(words) - 1):\n",
        "        x = (words[i][1])\n",
        "        if not x in Counts:\n",
        "            Counts[x] = 1\n",
        "        else:\n",
        "            Counts[x] += 1\n",
        "    return Counts\n",
        "\n",
        "def get_pos_bigram_count(words):\n",
        "    Counts = {}\n",
        "    for i in range(len(words) - 1):\n",
        "        x = (words[i][1], words[i+1][1])\n",
        "        if not x in Counts:\n",
        "            Counts[x] = 1\n",
        "        else:\n",
        "            Counts[x] += 1\n",
        "    return Counts\n",
        "\n",
        "\n",
        "def get_pos_unigram_probability(posUnigramCounts, totalUnigramCount, posUnigram, d):\n",
        "    try:\n",
        "        N_pos = posUnigramCounts[posUnigram]\n",
        "    except KeyError:\n",
        "        N_pos = 0\n",
        "    N_plus = len(posUnigramCounts)\n",
        "    N = totalUnigramCount\n",
        "    lambda_dot = d * N_plus / N\n",
        "    P_uni = 1 / N_plus\n",
        "    return ((max(N_pos - d, 0) / N) + lambda_dot * P_uni)\n",
        "    \n",
        "\n",
        "def get_pos_bigram_probability(posUnigramCounts, totalUnigramCount, posBigramCounts, posBigram, d):\n",
        "    try:\n",
        "        N_pos_Bigram = posBigramCounts[posBigram]\n",
        "    except KeyError:\n",
        "        N_pos_Bigram = 0\n",
        "    pos = posBigram[0]\n",
        "    N_pos = posUnigramCounts[pos]\n",
        "    history_pos =  posBigram[1]\n",
        "    try:\n",
        "        N_history = posUnigramCounts[history_pos]\n",
        "    except KeyError:\n",
        "        N_history = 0\n",
        "    N_plus_history = 0\n",
        "    for bigram in posBigramCounts:\n",
        "        if bigram[0][0] == history_pos:\n",
        "            N_plus_history = N_plus_history + 1\n",
        "    \n",
        "    P_abs = get_pos_unigram_probability(posUnigramCounts, totalUnigramCount, pos, d)\n",
        "    if N_history > 0:\n",
        "        lambda_history =   d * N_plus_history / N_history\n",
        "        prob = (max(N_pos_Bigram - d, 0) / N_history) + lambda_history * P_abs\n",
        "    else:\n",
        "        prob = P_abs\n",
        "    return prob\n",
        "\n",
        "\n",
        "def get_tagged_word_unigram_probability(unigramCounts, totalUnigramCount, taggedWord, d):\n",
        "    try:\n",
        "        N_w = unigramCounts[taggedWord[0]]\n",
        "    except KeyError:\n",
        "        N_w = 0\n",
        "    N = totalUnigramCount\n",
        "    P_uni = 1 / len(unigramCounts)\n",
        "    N_plus = len(unigramCounts)\n",
        "    lambda_dot = d * N_plus / N\n",
        "    prob = ((max(N_w - d, 0) / N) + lambda_dot * P_uni)\n",
        "    return prob\n",
        "    \n",
        "    \n",
        "def get_tagged_word_bigram_probability(unigramCounts, totalUnigramCount, posUnigramCounts, posBigramCounts, taggedWordCount, taggedWord, d):\n",
        "    try:    \n",
        "        N_w_pos = taggedWordCount[taggedWord]\n",
        "    except KeyError:\n",
        "        N_w_pos = 0\n",
        "    pos = taggedWord[1]\n",
        "    N_pos = posUnigramCounts[pos]\n",
        "    N_plus_pos = 0\n",
        "    for bigram in posBigramCounts:\n",
        "        if bigram[0][0] == pos:\n",
        "            N_plus_pos = N_plus_pos + 1\n",
        "    lambda_pos = d * N_plus_pos / N_pos\n",
        "    P_abs = get_tagged_word_unigram_probability(unigramCounts, totalUnigramCount, taggedWord, d)\n",
        "    if N_pos >0:\n",
        "        prob = (max(N_w_pos - d, 0) / N_pos) + lambda_pos * P_abs\n",
        "    else:\n",
        "        prob = P_abs\n",
        "    return prob\n",
        "\n",
        "def get_pos_list(taggedWords):\n",
        "    pos = []\n",
        "    for taggedWord in taggedWords:\n",
        "        if taggedWord[1] not in pos:\n",
        "            pos.append(taggedWord[1])\n",
        "    return pos\n",
        "\n",
        "def get_pos(word, posBigramCounts):\n",
        "    for bigram in posBigramCounts:\n",
        "        if bigram[0][0] == word:\n",
        "            return bigram[0][1]\n",
        "\n",
        "def get_class_based_bigram_probability(bigram, pos_list, unigramCounts, totalUnigramCount, posUnigramCounts, posBigramCounts, taggedWordCount, d, totalPosUnigramCount):\n",
        "    prob = 0\n",
        "    history_pos = get_pos(bigram[0], posBigramCounts)\n",
        "    for pos in pos_list:\n",
        "        taggedWord = (bigram[0], pos)\n",
        "        posBigram = (pos, history_pos)\n",
        "        taggedWordBigramProbability =  get_tagged_word_bigram_probability(unigramCounts, totalUnigramCount, posUnigramCounts, posBigramCounts, taggedWordCount, taggedWord, d)\n",
        "        posBigramProbability = get_pos_bigram_probability(posUnigramCounts, totalPosUnigramCount, posBigramCounts, posBigram, d)\n",
        "        prob = prob + (taggedWordBigramProbability * posBigramProbability)\n",
        "    return prob\n",
        "\n",
        "\"\"\"\n",
        "def compute_perplexity(interp = True):\n",
        "     sumlog = 0\n",
        "    for bigram in testBigrams:\n",
        "        word1 = bigram[0]\n",
        "         word2 = bigram[1]\n",
        "         if interp:\n",
        "             bigram_prob = estimate_interpolated_bigram_prob(word1, word2)\n",
        "         else:\n",
        "             bigram_prob = calculcate_bigram_prob(word1, word2)\n",
        "         log_bigram_prob = math.log(bigram_prob, 2)\n",
        "         sumlog = sumlog + log_bigram_prob\n",
        "     perplexity = pow(2, (-1) * sumlog/len(testBigrams))  \n",
        "     return perplexity \n",
        "\"\"\" \n",
        "\n",
        "def perplexity(testBigrams, pos_list, unigramCounts, totalUnigramCount, totalBigramCount, posUnigramCounts, posBigramCounts, taggedWordCount, d, totalPosUnigramCount):\n",
        "    entropy = 0\n",
        "    for bigram in testBigrams:\n",
        "        rel_prob = testBigrams[bigram] / totalBigramCount\n",
        "        smoothed_prob = get_class_based_bigram_probability(bigram, pos_list, unigramCounts, totalUnigramCount, posUnigramCounts, posBigramCounts, taggedWordCount, d,  totalPosUnigramCount)\n",
        "        log_smoothed_prob = math.log1p(smoothed_prob)\n",
        "        entropy = entropy + rel_prob * smoothed_prob\n",
        "    perplexity = math.pow(2, -entropy)\n",
        "    return perplexity"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MA9qyFgbX5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Driver Code\n",
        "corp = treebank.tagged_words()\n",
        "trainset, testset = pre_process(corp)\n",
        "print('Train Content: {}'.format(trainset))\n",
        "print('Test Content: {}\\n'.format(testset))\n",
        "\n",
        "#Trainset & Testset Word Counts\n",
        "trainWordCount = get_word_count(trainset)\n",
        "testWordCount = get_word_count(testset)\n",
        "\n",
        "#Trainset & Testset Word Unigrams\n",
        "trainWordUnigrams = get_word_unigram_count(trainset)\n",
        "testWordUnigrams = get_word_unigram_count(testset)\n",
        "\n",
        "#Trainset & Testset Word Bigrams\n",
        "trainWordBigrams = get_word_bigram_count(trainset)\n",
        "testWordBigrams = get_word_bigram_count(testset)\n",
        "\n",
        "#Trainset & Testset Pos Unigrams\n",
        "trainPosUnigrams = get_pos_unigram_count(trainset)\n",
        "testPosUnigrams = get_pos_unigram_count(testset)\n",
        "\n",
        "#Trainset & Testset Pos Bigrams\n",
        "trainPosBigrams = get_pos_bigram_count(trainset)\n",
        "testPosBigrams = get_pos_bigram_count(testset)\n",
        "\n",
        "print('Train Word Counts: {}'.format(trainWordCount))\n",
        "print('Train Word Unigram Counts: {}'.format(trainWordUnigrams))\n",
        "print('Train Word Bigram Counts: {}\\n'.format(trainWordBigrams))\n",
        "\n",
        "#Total Unigram Word & Pos counts\n",
        "trainTotalPosUnigramCount = total_count(trainPosUnigrams)\n",
        "trainTotalWordUnigramCount = total_count(trainWordUnigrams)\n",
        "\n",
        "#Total Bigram Word & Pos counts\n",
        "trainTotalPosBigramCount = total_count(trainPosBigrams)\n",
        "trainTotalWordBigramCount = total_count(trainWordBigrams)\n",
        "trainPosList = get_pos_list(trainset)\n",
        "\n",
        "#Bigram probailities for task B and D\n",
        "BigramProb = []\n",
        "for bigram in trainWordBigrams:\n",
        "  prob = get_class_based_bigram_probability(bigram[0], trainPosList, trainWordUnigrams, trainTotalWordUnigramCount, trainPosUnigrams, trainPosBigrams, trainWordCount, 0.9, trainTotalPosUnigramCount)\n",
        "  BigramProb.append((bigram[0], prob))\n",
        "print(BigramProb)\n",
        "\n",
        "#Perplexities for task C & D\n",
        "perplexity = perplexity(testWordBigrams, trainPosList, trainWordUnigrams, trainTotalWordUnigramCount, trainTotalWordBigramCount, trainPosUnigrams, trainPosBigrams, trainWordCount, 0.9, trainTotalPosUnigramCount)\n",
        "print(perplexity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlTGDkOQoSXL",
        "colab_type": "text"
      },
      "source": [
        "**Results for task b, c & d are printed on the fly in the above code cell. Thank you!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH4VANoAnYJS",
        "colab_type": "text"
      },
      "source": [
        "# Inference for task e:\n",
        "The perplexity value of the language model considering the POS tags is lower than the model without considering the tags. This would make it evident that the later has better performance in terms of predicting the sample space correctly!"
      ]
    }
  ]
}