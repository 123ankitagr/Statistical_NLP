{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNLP Assn 2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lrnT_gJTaUm",
        "colab_type": "text"
      },
      "source": [
        "# **SNLP Assignment 2**\n",
        "\n",
        "**Team Members:**\n",
        "\n",
        "\n",
        "*   Ankit Agrawal - 2581532\n",
        "*   Akshay Joshi - 2581346\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq6RLys_A1_z",
        "colab_type": "text"
      },
      "source": [
        "**Please run the code in Google Colab: https://colab.research.google.com/drive/1TM5oUnt0U0-WoW3hPIRohDdSZEvXIBxP?usp=sharing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ruWbqOS3FW",
        "colab_type": "text"
      },
      "source": [
        "We had few issues with the envirnment and memory in our local systems. So, we wrote our code in Jupyter notebook.\n",
        "Sorry if this was not allowed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfopOmIp7zhB",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1.1 & 1.2 (combined):**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqc9fig97EEK",
        "colab_type": "text"
      },
      "source": [
        "**For the sake of Easy understadability, we are creating kernels to achieve same tasks over different corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fpsGrWcSEs2",
        "colab_type": "text"
      },
      "source": [
        "**!Please scroll below Question 1's code for report!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQysJleUH253",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from pylab import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrtoaj8rFE7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = ['/content/en/ankara.txt', '/content/en/bandung.txt', '/content/en/bangkok.txt', '/content/en/barcelona.txt', '/content/en/beijing.txt', '/content/en/berlin.txt', '/content/en/bogota.txt', '/content/en/bombay.txt', '/content/en/boston.txt', '/content/en/budapest.txt', '/content/en/cape-town.txt', '/content/en/chicago.txt', '/content/en/delhi.txt', '/content/en/dublin.txt', '/content/en/edinburgh.txt', '/content/en/glasgow.txt', '/content/en/hamburg.txt', '/content/en/hongkong.txt',  '/content/en/istanbul.txt', '/content/en/jakarta.txt', '/content/en/karachi.txt', '/content/en/kinshasa.txt', '/content/en/lima.txt', '/content/en/los-angeles.txt', '/content/en/madrid.txt', '/content/en/melbourne.txt', '/content/en/mexico-city.txt', '/content/en/montreal.txt', '/content/en/mumbai.txt', '/content/en/munich.txt', '/content/en/nagoya.txt', '/content/en/osaka.txt', '/content/en/rio-de-janeiro.txt', '/content/en/san-paulo.txt', '/content/en/seattle.txt', '/content/en/seoul.txt', '/content/en/shanghai.txt', '/content/en/st-petersburg.txt', '/content/en/sydney.txt', '/content/en/tehran.txt', '/content/en/tel-aviv.txt', '/content/en/tianjin.txt', '/content/en/toronto.txt', '/content/en/yokohama.txt'] \n",
        "with open('/content/Corpus/en_corpus.txt', 'w') as outfile: \n",
        "  for names in filenames:  \n",
        "    with open(names) as infile:  \n",
        "      outfile.write(infile.read()) \n",
        "    outfile.write(\"\\n\") \n",
        "\n",
        "\n",
        "filenames = ['/content/de/ankara.txt', '/content/de/bandung.txt', '/content/de/bangkok.txt', '/content/de/barcelona.txt', '/content/de/beijing.txt', '/content/de/berlin.txt', '/content/de/bogota.txt', '/content/de/bombay.txt', '/content/de/boston.txt', '/content/de/budapest.txt', '/content/de/cape-town.txt', '/content/de/chicago.txt', '/content/de/delhi.txt', '/content/de/dublin.txt', '/content/de/edinburgh.txt', '/content/de/glasgow.txt', '/content/de/hamburg.txt', '/content/de/hongkong.txt',  '/content/de/istanbul.txt', '/content/de/jakarta.txt', '/content/de/karachi.txt', '/content/de/kinshasa.txt', '/content/de/lima.txt', '/content/de/los-angeles.txt', '/content/de/madrid.txt', '/content/de/melbourne.txt', '/content/de/mexico-city.txt', '/content/de/montreal.txt', '/content/de/mumbai.txt', '/content/de/munich.txt', '/content/de/nagoya.txt', '/content/de/osaka.txt', '/content/de/rio-de-janeiro.txt', '/content/de/san-paulo.txt', '/content/de/seattle.txt', '/content/de/seoul.txt', '/content/de/shanghai.txt', '/content/de/st-petersburg.txt', '/content/de/sydney.txt', '/content/de/tehran.txt', '/content/de/tel-aviv.txt', '/content/de/tianjin.txt', '/content/de/toronto.txt', '/content/de/yokohama.txt'] \n",
        "with open('/content/Corpus/de_corpus.txt', 'w') as outfile: \n",
        "  for names in filenames:  \n",
        "    with open(names) as infile:  \n",
        "      outfile.write(infile.read()) \n",
        "    outfile.write(\"\\n\") \n",
        "\n",
        "\n",
        "filenames = ['/content/es/ankara.txt', '/content/es/bandung.txt', '/content/es/bangkok.txt', '/content/es/barcelona.txt', '/content/es/beijing.txt', '/content/es/berlin.txt', '/content/es/bogota.txt', '/content/es/bombay.txt', '/content/es/boston.txt', '/content/es/budapest.txt', '/content/es/cape-town.txt', '/content/es/chicago.txt', '/content/es/delhi.txt', '/content/es/dublin.txt', '/content/es/edinburgh.txt', '/content/es/glasgow.txt', '/content/es/hamburg.txt', '/content/es/hongkong.txt',  '/content/es/istanbul.txt', '/content/es/jakarta.txt', '/content/es/karachi.txt', '/content/es/kinshasa.txt', '/content/es/lima.txt', '/content/es/los-angeles.txt', '/content/es/madrid.txt', '/content/es/melbourne.txt', '/content/es/mexico-city.txt', '/content/es/montreal.txt', '/content/es/mumbai.txt', '/content/es/munich.txt', '/content/es/nagoya.txt', '/content/es/osaka.txt', '/content/es/rio-de-janeiro.txt', '/content/es/san-paulo.txt', '/content/es/seattle.txt', '/content/es/seoul.txt', '/content/es/shanghai.txt', '/content/es/st-petersburg.txt', '/content/es/sydney.txt', '/content/es/tehran.txt', '/content/es/tel-aviv.txt', '/content/es/tianjin.txt', '/content/es/toronto.txt', '/content/es/yokohama.txt'] \n",
        "with open('/content/Corpus/es_corpus.txt', 'w') as outfile: \n",
        "  for names in filenames:  \n",
        "    with open(names) as infile:  \n",
        "      outfile.write(infile.read()) \n",
        "    outfile.write(\"\\n\")\n",
        "\n",
        "filenames = ['/content/hu/ankara.txt', '/content/hu/bandung.txt', '/content/hu/bangkok.txt', '/content/hu/barcelona.txt', '/content/hu/beijing.txt', '/content/hu/berlin.txt', '/content/hu/bogota.txt', '/content/hu/bombay.txt', '/content/hu/boston.txt', '/content/hu/budapest.txt', '/content/hu/cape-town.txt', '/content/hu/chicago.txt', '/content/hu/delhi.txt', '/content/hu/dublin.txt', '/content/hu/edinburgh.txt', '/content/hu/glasgow.txt', '/content/hu/hamburg.txt', '/content/hu/hongkong.txt',  '/content/hu/istanbul.txt', '/content/hu/jakarta.txt', '/content/hu/karachi.txt', '/content/hu/kinshasa.txt', '/content/hu/lima.txt', '/content/hu/los-angeles.txt', '/content/hu/madrid.txt', '/content/hu/melbourne.txt', '/content/hu/mexico-city.txt', '/content/hu/montreal.txt', '/content/hu/mumbai.txt', '/content/hu/munich.txt', '/content/hu/nagoya.txt', '/content/hu/osaka.txt', '/content/hu/rio-de-janeiro.txt', '/content/hu/san-paulo.txt', '/content/hu/seattle.txt', '/content/hu/seoul.txt', '/content/hu/shanghai.txt', '/content/hu/st-petersburg.txt', '/content/hu/sydney.txt', '/content/hu/tehran.txt', '/content/hu/tel-aviv.txt', '/content/hu/tianjin.txt', '/content/hu/toronto.txt', '/content/hu/yokohama.txt'] \n",
        "with open('/content/Corpus/hu_corpus.txt', 'w') as outfile: \n",
        "  for names in filenames:  \n",
        "    with open(names) as infile:  \n",
        "      outfile.write(infile.read()) \n",
        "    outfile.write(\"\\n\")\n",
        "\n",
        "filenames = ['/content/tr/ankara.txt', '/content/tr/bandung.txt', '/content/tr/bangkok.txt', '/content/tr/barcelona.txt', '/content/tr/beijing.txt', '/content/tr/berlin.txt', '/content/tr/bogota.txt', '/content/tr/bombay.txt', '/content/tr/boston.txt', '/content/tr/budapest.txt', '/content/tr/cape-town.txt', '/content/tr/chicago.txt', '/content/tr/delhi.txt', '/content/tr/dublin.txt', '/content/tr/edinburgh.txt', '/content/tr/glasgow.txt', '/content/tr/hamburg.txt', '/content/tr/hongkong.txt',  '/content/tr/istanbul.txt', '/content/tr/jakarta.txt', '/content/tr/karachi.txt', '/content/tr/kinshasa.txt', '/content/tr/lima.txt', '/content/tr/los-angeles.txt', '/content/tr/madrid.txt', '/content/tr/melbourne.txt', '/content/tr/mexico-city.txt', '/content/tr/montreal.txt', '/content/tr/mumbai.txt', '/content/tr/munich.txt', '/content/tr/nagoya.txt', '/content/tr/osaka.txt', '/content/tr/rio-de-janeiro.txt', '/content/tr/san-paulo.txt', '/content/tr/seattle.txt', '/content/tr/seoul.txt', '/content/tr/shanghai.txt', '/content/tr/st-petersburg.txt', '/content/tr/sydney.txt', '/content/tr/tehran.txt', '/content/tr/tel-aviv.txt', '/content/tr/tianjin.txt', '/content/tr/toronto.txt', '/content/tr/yokohama.txt'] \n",
        "with open('/content/Corpus/tr_corpus.txt', 'w') as outfile: \n",
        "  for names in filenames:  \n",
        "    with open(names) as infile:  \n",
        "      outfile.write(infile.read()) \n",
        "    outfile.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmO3tcrQhZz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For EN Corpus\n",
        "\n",
        "corpus_file = '/content/Corpus/en_corpus.txt'\n",
        "file = open(corpus_file, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "text1 = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text2 = text1.translate(str.maketrans('','','一■–ー\\n\\t'))\n",
        "text3 = text2.lower()\n",
        "words = text3.split()\n",
        "print(words[:250])\n",
        "with open('/content/Corpus/en_processed.txt', 'w') as ff:\n",
        "  for items in words:\n",
        "    ff.write(\"%s \\n\" %items)\n",
        "\n",
        "#Count/Frequency of Top 10 words\n",
        "uniques = []\n",
        "for word in words:\n",
        "  if word not in uniques:\n",
        "    uniques.append(word)\n",
        "\n",
        "counts = []\n",
        "top = []\n",
        "for unique in uniques:\n",
        "  count = 0         \n",
        "  for word in words:\n",
        "    if word == unique: \n",
        "      count += 1   \n",
        "  counts.append((count, unique))\n",
        "\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "\n",
        "# Print Top-10 words with the highest counts\n",
        "for i in range(min(10, len(counts))):\n",
        "  count, word = counts[i]\n",
        "  print('%s %d' % (word, count))\n",
        "  top.append(word)\n",
        "\n",
        "#Average length of Top-10 words\n",
        "top10average = sum(len(word) for word in top) / len(top)\n",
        "print(\"Average Length of Top-10 words:\" + str(top10average))\n",
        "\n",
        "#Average length of Low ranked words\n",
        "bottom = []\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  if count == 1:\n",
        "    bottom.append(word)\n",
        "\n",
        "#Average of low ranked words\n",
        "bottomaverage = sum(len(word) for word in bottom) / len(bottom)\n",
        "print(\"Average Length of Low ranked words:\" + str(bottomaverage))\n",
        "\n",
        "#Finding size of smallest corpus (which is Turkish)\n",
        "smallest_corpus = '/content/Corpus/tr_corpus.txt'\n",
        "file = open(smallest_corpus, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "text1 = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text2 = text1.translate(str.maketrans('','','一■–ー\\n\\t'))\n",
        "text3 = text2.lower()\n",
        "smallest_words = text3.split()\n",
        "smallest_length = len(smallest_words)\n",
        "\n",
        "#Normalize\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  count = ((count / len(counts)) * smallest_length)\n",
        "\n",
        "#Plot Zipf's distribution\n",
        "ranked = sorted(counts)\n",
        "loglog(ranked, count, marker=\".\")\n",
        "title(\"Zipf plot for EN\")\n",
        "xlabel(\"Rank of the word\")\n",
        "ylabel(\"Frequency of the word\")\n",
        "grid(True)\n",
        "for n in list(logspace(-0.5, log10(len(counts)), 20).astype(int)):\n",
        "    temp = text(ranked[n], count[n], verticalalignment=\"bottom\", horizontalalignment=\"left\")\n",
        "show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OedK5cgzlTXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For DE Corpus\n",
        "\n",
        "corpus_file = '/content/Corpus/de_corpus.txt'\n",
        "file = open(corpus_file, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "text1 = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text2 = text1.translate(str.maketrans('','','一■–ー\\n\\t'))\n",
        "text3 = text2.lower()\n",
        "words = text3.split()\n",
        "print(words[:250])\n",
        "with open('/content/Corpus/de_processed.txt', 'w') as ff:\n",
        "  for items in words:\n",
        "    ff.write(\"%s \\n\" %items)\n",
        "\n",
        "#Count/Frequency of Top 10 words\n",
        "uniques = []\n",
        "for word in words:\n",
        "  if word not in uniques:\n",
        "    uniques.append(word)\n",
        "\n",
        "counts = []\n",
        "top = []\n",
        "for unique in uniques:\n",
        "  count = 0         \n",
        "  for word in words:\n",
        "    if word == unique: \n",
        "      count += 1   \n",
        "  counts.append((count, unique))\n",
        "\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "\n",
        "# Print Top-10 words with the highest counts\n",
        "for i in range(min(10, len(counts))):\n",
        "  count, word = counts[i]\n",
        "  print('%s %d' % (word, count))\n",
        "  top.append(word)\n",
        "\n",
        "#Average length of Top-10 words\n",
        "top10average = sum(len(word) for word in top) / len(top)\n",
        "print(\"Average Length of Top-10 words:\" + str(top10average))\n",
        "\n",
        "#Average length of Low ranked words\n",
        "bottom = []\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  if count == 1:\n",
        "    bottom.append(word)\n",
        "\n",
        "#Average of low ranked words\n",
        "bottomaverage = sum(len(word) for word in bottom) / len(bottom)\n",
        "print(\"Average Length of Low ranked words:\" + str(bottomaverage))\n",
        "\n",
        "#Normalize\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  count = ((count / len(counts)) * smallest_length)\n",
        "\n",
        "#Plot Zipf's distribution\n",
        "ranked = sorted(counts)\n",
        "loglog(ranked, count, marker=\".\")\n",
        "title(\"Zipf plot for DE\")\n",
        "xlabel(\"Rank of the word\")\n",
        "ylabel(\"Frequency of the word\")\n",
        "grid(True)\n",
        "for n in list(logspace(-0.5, log10(len(counts)), 20).astype(int)):\n",
        "    temp = text(ranked[n], count[n], verticalalignment=\"bottom\", horizontalalignment=\"left\")\n",
        "show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn1yiEa8lj1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For ES Corpus\n",
        "\n",
        "corpus_file = '/content/Corpus/es_corpus.txt'\n",
        "file = open(corpus_file, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "text1 = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text2 = text1.translate(str.maketrans('','','一■–ー«»\\n\\t\\u200b'))\n",
        "text3 = text2.lower()\n",
        "words = text3.split()\n",
        "print(words[:250])\n",
        "with open('/content/Corpus/es_processed.txt', 'w') as ff:\n",
        "  for items in words:\n",
        "    ff.write(\"%s \\n\" %items)\n",
        "\n",
        "#Count/Frequency of Top 10 words\n",
        "uniques = []\n",
        "for word in words:\n",
        "  if word not in uniques:\n",
        "    uniques.append(word)\n",
        "\n",
        "counts = []\n",
        "top = []\n",
        "for unique in uniques:\n",
        "  count = 0         \n",
        "  for word in words:\n",
        "    if word == unique: \n",
        "      count += 1   \n",
        "  counts.append((count, unique))\n",
        "\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "\n",
        "# Print Top-10 words with the highest counts\n",
        "for i in range(min(10, len(counts))):\n",
        "  count, word = counts[i]\n",
        "  print('%s %d' % (word, count))\n",
        "  top.append(word)\n",
        "\n",
        "#Average length of Top-10 words\n",
        "top10average = sum(len(word) for word in top) / len(top)\n",
        "print(\"Average Length of Top-10 words:\" + str(top10average))\n",
        "\n",
        "#Average length of Low ranked words\n",
        "bottom = []\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  if count == 1:\n",
        "    bottom.append(word)\n",
        "\n",
        "#Average of low ranked words\n",
        "bottomaverage = sum(len(word) for word in bottom) / len(bottom)\n",
        "print(\"Average Length of Low ranked words:\" + str(bottomaverage))\n",
        "\n",
        "#Normalize\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  count = ((count / len(counts)) * smallest_length)\n",
        "\n",
        "#Plot Zipf's distribution\n",
        "ranked = sorted(counts)\n",
        "loglog(ranked, count, marker=\".\")\n",
        "title(\"Zipf plot for ES\")\n",
        "xlabel(\"Rank of the word\")\n",
        "ylabel(\"Frequency of the word\")\n",
        "grid(True)\n",
        "for n in list(logspace(-0.5, log10(len(counts)), 20).astype(int)):\n",
        "    temp = text(ranked[n], count[n], verticalalignment=\"bottom\", horizontalalignment=\"left\")\n",
        "show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD-PZe4zmQuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For HU Corpus\n",
        "\n",
        "corpus_file = '/content/Corpus/hu_corpus.txt'\n",
        "file = open(corpus_file, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "text1 = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text2 = text1.translate(str.maketrans('','','一■–ー\\n\\t'))\n",
        "text3 = text2.lower()\n",
        "words = text3.split()\n",
        "print(words[:250])\n",
        "with open('/content/Corpus/hu_processed.txt', 'w') as ff:\n",
        "  for items in words:\n",
        "    ff.write(\"%s \\n\" %items)\n",
        "\n",
        "\n",
        "#Count/Frequency of Top 10 words\n",
        "uniques = []\n",
        "for word in words:\n",
        "  if word not in uniques:\n",
        "    uniques.append(word)\n",
        "\n",
        "counts = []\n",
        "top = []\n",
        "for unique in uniques:\n",
        "  count = 0         \n",
        "  for word in words:\n",
        "    if word == unique: \n",
        "      count += 1   \n",
        "  counts.append((count, unique))\n",
        "\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "\n",
        "# Print Top-10 words with the highest counts\n",
        "for i in range(min(10, len(counts))):\n",
        "  count, word = counts[i]\n",
        "  print('%s %d' % (word, count))\n",
        "  top.append(word)\n",
        "\n",
        "#Average length of Top-10 words\n",
        "top10average = sum(len(word) for word in top) / len(top)\n",
        "print(\"Average Length of Top-10 words:\" + str(top10average))\n",
        "\n",
        "#Average length of Low ranked words\n",
        "bottom = []\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  if count == 1:\n",
        "    bottom.append(word)\n",
        "\n",
        "#Average of low ranked words\n",
        "bottomaverage = sum(len(word) for word in bottom) / len(bottom)\n",
        "print(\"Average Length of Low ranked words:\" + str(bottomaverage))\n",
        "\n",
        "#Normalize\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  count = ((count / len(counts)) * smallest_length)\n",
        "\n",
        "#Plot Zipf's distribution\n",
        "ranked = sorted(counts)\n",
        "loglog(ranked, count, marker=\".\")\n",
        "title(\"Zipf plot for HU\")\n",
        "xlabel(\"Rank of the word\")\n",
        "ylabel(\"Frequency of the word\")\n",
        "grid(True)\n",
        "for n in list(logspace(-0.5, log10(len(counts)), 20).astype(int)):\n",
        "    temp = text(ranked[n], count[n], verticalalignment=\"bottom\", horizontalalignment=\"left\")\n",
        "show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZimRoCumZe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For TR Corpus\n",
        "\n",
        "corpus_file = '/content/Corpus/tr_corpus.txt'\n",
        "file = open(corpus_file, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "text1 = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text2 = text1.translate(str.maketrans('','','一■–ー\\n\\t'))\n",
        "text3 = text2.lower()\n",
        "words = text3.split()\n",
        "print(words[:250])\n",
        "with open('/content/Corpus/tr_processed.txt', 'w') as ff:\n",
        "  for items in words:\n",
        "    ff.write(\"%s \\n\" %items)\n",
        "\n",
        "#Count/Frequency of Top 10 words\n",
        "uniques = []\n",
        "for word in words:\n",
        "  if word not in uniques:\n",
        "    uniques.append(word)\n",
        "\n",
        "counts = []\n",
        "top = []\n",
        "for unique in uniques:\n",
        "  count = 0         \n",
        "  for word in words:\n",
        "    if word == unique: \n",
        "      count += 1   \n",
        "  counts.append((count, unique))\n",
        "\n",
        "counts.sort()\n",
        "counts.reverse()\n",
        "\n",
        "# Print Top-10 words with the highest counts\n",
        "for i in range(min(10, len(counts))):\n",
        "  count, word = counts[i]\n",
        "  print('%s %d' % (word, count))\n",
        "  top.append(word)\n",
        "\n",
        "#Average length of Top-10 words\n",
        "top10average = sum(len(word) for word in top) / len(top)\n",
        "print(\"Average Length of Top-10 words:\" + str(top10average))\n",
        "\n",
        "#Average length of Low ranked words\n",
        "bottom = []\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  if count == 1:\n",
        "    bottom.append(word)\n",
        "\n",
        "#Average of low ranked words\n",
        "bottomaverage = sum(len(word) for word in bottom) / len(bottom)\n",
        "print(\"Average Length of Low ranked words:\" + str(bottomaverage))\n",
        "\n",
        "#Normalize\n",
        "for i in range(len(counts)):\n",
        "  count, word = counts[i]\n",
        "  count = ((count / len(counts)) * smallest_length)\n",
        "\n",
        "#Plot Zipf's distribution\n",
        "ranked = sorted(counts)\n",
        "loglog(ranked, count, marker=\".\")\n",
        "title(\"Zipf plot for TR\")\n",
        "xlabel(\"Rank of the word\")\n",
        "ylabel(\"Frequency of the word\")\n",
        "grid(True)\n",
        "for n in list(logspace(-0.5, log10(len(counts)), 20).astype(int)):\n",
        "    temp = text(ranked[n], count[n], verticalalignment=\"bottom\", horizontalalignment=\"left\")\n",
        "show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T64-Gl0ZTSjl",
        "colab_type": "text"
      },
      "source": [
        "# **Report:**\n",
        "**1.1 Data Preprocessing**\n",
        "\n",
        "\n",
        "**English:**\n",
        "\n",
        "The top 10 words are:\n",
        " the, of, and, in, to, a, is, city, as, was\n",
        "\n",
        " Avg(Top 10): 2.5\n",
        "\n",
        " Avg(Low rank words): 9.4\n",
        "\n",
        "\n",
        " \n",
        "**DE:**\n",
        "\n",
        "The top 10 words are:\n",
        "'der', 'die', 'und', 'in', 'von', 'im', 'den', 'des', 'das', 'mit'\n",
        "\n",
        " Avg(Top 10): 2.8\n",
        "\n",
        " Avg(Low rank words): 10.234\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "**ES:**\n",
        "\n",
        "The top 10 words are:\n",
        "'de', 'la', 'el', 'en', 'y', 'del', 'a', 'se', 'los', 'que'\n",
        "\n",
        " Avg(Top 10): 2.1\n",
        "\n",
        " Avg(Low rank words): 9.2187\n",
        "\n",
        "\n",
        " \n",
        "**HU:**\n",
        "\n",
        "The top 10 words are:\n",
        "'a', 'az', 'és', 'is', 'város', 'volt', 'de', 'egy', 'meg' ‘legnagyobb’\n",
        "\n",
        " Avg(Top 10): 3.5\n",
        "\n",
        " Avg(Low rank words): 9.1965\n",
        "\n",
        "\n",
        " \n",
        "**TR:**\n",
        "\n",
        "The top 10 words are:\n",
        "'ve', 'bir', 'bu', 'en', 'de', 'olarak', 'büyük', 'ile', 'da', 'olan'\n",
        "\n",
        " Avg(Top 10): 3.0\n",
        "\n",
        " Avg(Low rank words): 9.2\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}