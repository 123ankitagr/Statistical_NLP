{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNLP Assn 8 Q2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyNS7O0HeUcy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "52ad1b65-7185-4256-8d2c-d679fa12aaf4"
      },
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import glob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import wordpunct_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMdxPvFVqx0c",
        "colab_type": "text"
      },
      "source": [
        "**Task 2.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54llTiK2d0B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combine different Positive and Negative Review text files into concatinated Postive_Result & Negative_Result text files\n",
        "from nltk.util import ngrams\n",
        "def Combine_Positive():\n",
        "  #Should be run in Pos reviews directory\n",
        "  import glob\n",
        "  read_files = glob.glob(\"/content/pos/*.txt\")\n",
        "  with open(\"Positive_result.txt\", \"w\") as outfile:\n",
        "    for f in read_files:\n",
        "      with open(f, \"r\", errors='ignore') as infile:\n",
        "        outfile.write(infile.read()+'\\n')\n",
        "    return\n",
        "  \n",
        "\n",
        "def Combine_Negative():\n",
        "  #Should be run in Neg reviews directory\n",
        "  import glob\n",
        "  read_files = glob.glob(\"/content/neg/*.txt\")\n",
        "  with open(\"Negative_result.txt\", \"w\") as outfile:\n",
        "    for f in read_files:\n",
        "      with open(f, \"r\", errors='ignore') as infile:\n",
        "        outfile.write(infile.read()+'\\n')\n",
        "    return\n",
        "\n",
        "def tokenize(text):\n",
        "  text = text.lower() \n",
        "  words = wordpunct_tokenize(text)\n",
        "  words_cleaned = [word for word in words if word.isalpha()]\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  final_words = [word for word in words_cleaned if not word in stop_words]\n",
        "  return final_words\n",
        "\n",
        "\"\"\"\n",
        "#These commands may crash the browser if run on Google Colab as the functions \"Combine Positive and Negative\"  need around \n",
        "## 30,000 txt files loaded to the instance before actually running the function.\n",
        "pos = Combine_Positive()\n",
        "neg = Combine_Negative()\n",
        "\"\"\"\n",
        "\n",
        "#So Loading already concatenated file (generated on local PC)\n",
        "f = open(\"/content/resultPOS.txt\", \"r\", errors='ignore')\n",
        "pos = f.read()\n",
        "f.close()\n",
        "\n",
        "f = open(\"/content/resultNEG.txt\", \"r\", errors='ignore')\n",
        "neg = f.read()\n",
        "f.close()\n",
        "\n",
        "#Tokenize and remove stopwords using NTLK corpus\n",
        "pos_tok = tokenize(pos)\n",
        "neg_tok = tokenize(neg)\n",
        "print(pos_tok[:50])\n",
        "print(neg_tok[:50])\n",
        "\n",
        "#Generating Unigrams & Bigrams for Pos and Neg reviews\n",
        "Pos_Unigrams = pos_tok\n",
        "Pos_Bigrams = ngrams(Pos_Unigrams, 2)\n",
        "Neg_Unigrams = neg_tok\n",
        "Neg_Bigrams = ngrams(Neg_Unigrams, 2)\n",
        "\n",
        "#Applying CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "pos_vectors = vectorizer.fit_transform(pos_tok)\n",
        "neg_vectors = vectorizer.fit_transform(neg_tok)\n",
        "X, y = make_classification(n_features=4)\n",
        "clf = LinearSVC(C=0.5)\n",
        "clf.fit(X, y)\n",
        "#text_pred = Pos_Unigrams.fit(pos, clf)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXIL5HcQrvWE",
        "colab_type": "text"
      },
      "source": [
        "Task 2.2\n",
        "\n",
        "\n",
        "We are using Decision Tree (non-parametric supervised learning method used for classification and regression)\n",
        "We have found out that SVM are more robust than simple Decision Trees\n"
      ]
    }
  ]
}