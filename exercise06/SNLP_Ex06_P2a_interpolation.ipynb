{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNLP_Ex06_P2a_interpolation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VR3seftaO2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5juh7O7_ccNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = {}\n",
        "bigrams = {}\n",
        "testUnigrams = {}\n",
        "testBigrams = {}\n",
        "\n",
        "with open('English_train.txt', 'r', encoding='utf8') as f:\n",
        "    train_lines = f.readlines()\n",
        "\n",
        "with open('English_test.txt', 'r', encoding='utf8') as f:\n",
        "    test_lines = f.readlines()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fiuz0WYXcMGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
        "    return re.findall('[a-z]+', text.lower())"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uqn1xZdabOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_ngrams(sent, n):\n",
        "    \"\"\"Given a sent as str return n-grams as a list of tuple\"\"\"\n",
        "\n",
        "    tp=()\n",
        "    listtp= []\n",
        "    token = tokenize(sent)\n",
        "    #if it is not 1-gram append <s> to the begining and </s> at the end\n",
        "    if(n!=1):\n",
        "        token.insert(0,\"<s>\")\n",
        "        token.append(\"</s>\")\n",
        "    \n",
        "    #generate n-grams using the zip function in python\n",
        "    ngrams = zip(*[token[i:] for i in range(n)])\n",
        "    \n",
        "    #loop through the n-grams generated and create list of tuples\n",
        "    for ngram in ngrams:\n",
        "        listtp.append(ngram)\n",
        "        \n",
        "    return listtp\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi3pUo2EaxQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#helper funcion fr ngrams\n",
        "def getGram(lines, n):\n",
        "  bigramst=[]\n",
        "  if type(lines) == list:\n",
        "    for sentences in lines:\n",
        "      bg=word_ngrams(sentences, n)\n",
        "      bigramst+=bg\n",
        "  else:\n",
        "    bigramst = word_ngrams(lines, n)\n",
        "  return bigramst"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imr8qQV4gIRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v_unigrams = getGram(train_lines, 1)\n",
        "v_bigrams = getGram(train_lines, 2)\n",
        "unigrams=Counter(v_unigrams)\n",
        "bigrams=Counter(v_bigrams)\n",
        "\n",
        "tv_unigrams = getGram(test_lines, 1)\n",
        "tv_bigrams = getGram(test_lines, 2)\n",
        "testUnigrams=Counter(tv_unigrams)\n",
        "testBigrams=Counter(tv_bigrams)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5OlLfutMDdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculcate_unigram_prob(word, alpha = 0.3):\n",
        "    N = len(wordTokens)\n",
        "    V = len(unigrams)\n",
        "    word_count = unigrams[word]\n",
        "\n",
        "    unigram_prob = (word_count + alpha)/((alpha * V)+ N)\n",
        "    return unigram_prob\n",
        "\n",
        "def calculcate_bigram_prob(word1, word2, alpha = 0.3):\n",
        "    w1_count = unigrams[word1]\n",
        "    V = len(unigrams)\n",
        "    bigram_count = bigrams[(word1, word2)] \n",
        "    bigram_prob = (bigram_count + alpha)/((alpha * V)+ w1_count)\n",
        "    return bigram_prob\n",
        "\n",
        "def estimate_interpolated_bigram_prob(word1, word2, lamda1 = 0.5, lamda2 = 0.5):\n",
        "    prob_w2 = calculcate_unigram_prob(word2)\n",
        "    prob_w1w2 = calculcate_bigram_prob(word1, word2)\n",
        "    interpolated_bigram_prob =   (lamda1 * prob_w2) + (lamda2 * prob_w1w2)\n",
        "    return interpolated_bigram_prob\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhApRbKggMTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5bff60ef-c329-44c2-b379-1c992689d987"
      },
      "source": [
        "def compute_perplexity(interp = True):\n",
        "    sumlog = 0\n",
        "    for bigram in testBigrams:\n",
        "        word1 = bigram[0]\n",
        "        word2 = bigram[1]\n",
        "        if interp:\n",
        "            bigram_prob = estimate_interpolated_bigram_prob(word1, word2)\n",
        "        else:\n",
        "            bigram_prob = calculcate_bigram_prob(word1, word2)\n",
        "        log_bigram_prob = math.log(bigram_prob, 2)\n",
        "        sumlog = sumlog + log_bigram_prob\n",
        "    perplexity = pow(2, (-1) * sumlog/len(testBigrams))  \n",
        "    return perplexity          \n",
        "    \n",
        "print(\"Perplexity with Interpolation: \", compute_perplexity(True))\n",
        "print(\"Perplexity no Interpolation: \", compute_perplexity(False))\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity with Interpolation:  3490.97982265205\n",
            "Perplexity no Interpolation:  1763.4299598123403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p-6SRuyhF94",
        "colab_type": "text"
      },
      "source": [
        "Here we can see the perplexity values are less for Listone smoothing without intepolation.\n",
        "Limestone smoothing is not efficient two counts can be 0. so we need interpolation as it consider unigram. \n"
      ]
    }
  ]
}